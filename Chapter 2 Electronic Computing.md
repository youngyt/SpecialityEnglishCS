In Chapter One, we knew about special purpose computing devices like tabulating machines. They were a huge [[boon]] to governments and business - aiding and sometimes replacing [[rote]] manual [[tasks]]. But the scale of human systems continued to increase at an [[unprecedented]] rate. World War I **mobilized** 70 million people, and World War II involved more than 100 million. Global trade and **transit** **networks** became **interconnected** like never before, and the **sophistication** of our engineering and scientific endeavors reached new heights - we even started to seriously consider visiting other planets. And it was this explosion of **complexity**, **bureaucracy**, and data that drove an increasing need for automation and computation.

Soon those cabinet-sized **electro-mechanical** computers grew into room-sized **behemoths** that were expensive to maintain and **prone to** errors. And it was these machines that would set the stage for future innovation. One of the largest electro-mechanical computers built was the **_Harvard Mark I_**, completed in 1944 by IBM for the Allies during World War II. It contained 765,000 components, three million connections, and five hundred miles of wire. To keep its internal mechanics **synchronized**, it used a 50-foot **shaft** running right through the machine driven by a five horsepower **motor**. One of the earliest uses for this technology was running simulations for the **_Manhattan Project_**.           

The brains of these huge electro-mechanical beasts were **relays**: _electrically-controlled mechanical switches_. In a relay, there is a control wire that determines whether a circuit is opened or closed. The control wire connects to **a coil of** wire inside the relay. When **current** flows through the coil, an electromagnetic field is created, which in turn, attracts a metal arm inside the relay, **snapping** it shut and completing the **circuit**. The controlled circuit can then connect to other circuits, or to something like a motor, which might **increment** a count on a gear, like in Hollerith's tabulating machine.

Unfortunately, the mechanical arm inside of a relay has **mass** and therefore can't move **instantly** between opened and closed states. A good relay in the 1940's might be able to **flick back and forth** fifty times in a second. That might seem pretty fast, but it's not fast enough to **be useful at** solving large, complex problems. The Harvard Mark I could do 3 additions or subtractions per second; multiplications took 6 seconds, and divisions took 15. And more complex operations, like a trigonometric function, could take over a minute.
           
In addition to slow switching speed, another limitation was **wear and tear**. Anything mechanical that moves will wear over time. Some things break entirely, and other things start getting sticky, slow, and just plain unreliable. And as the number of relays increases, the **probability** of a failure increases too. The Harvard Mark I had **roughly** 3500 relays. Even if you assume a relay has an operational life of 10 years, this would mean you'd have to replace, on average, one faulty relay every day! That's a big problem when you are in the middle of running some important, multi-day calculation.

And that's not all engineers had to **contend with**. These huge, dark, and warm machines also attracted insects. In September 1947, operators on the **_Harvard Mark II_** pulled a dead moth from a **malfunctioning** relay. **_Grace Hopper_** noted: "_From then on, when anything went wrong with a computer,__we said it had bugs in it._" And that's where we get the term **computer bug**.

It was clear that a faster, more reliable alternative to electro-mechanical relays was needed if computing was going to advance further. Fortunately, that alternative had already existed!           

In 1904, English physicist **_John Ambrose Fleming_** developed a new electrical component called a **thermionic valve**, which housed two **electrodes** inside an airtight glass **bulb**- this was the first **vacuum tube**. One of the electrodes could be heated, which would cause it to emit electrons– a process called **_thermionic_** _**emission**_. The other electrode could then attract these **electrons** to create the flow of our electric faucet, but only if it was positively charged- if it had a negative or neutral **charge**, the electrons would no longer be attracted across the vacuum, so no current would flow.

An electronic component that permits the **one-way** flow of current is called a **diode**, but what was really needed was a switch to help turn this flow on and off. Luckily, in 1906, American inventor **_Lee de Forest_** added a third "control" electrode that sits between the two electrodes in Fleming's design. By applying a positive charge to the control electrode, it would permit the flow of electrons as before. But if the control electrode was given a negative charge, it would prevent the flow of electrons. So by manipulating the control wire, one could open or close the circuit. It's pretty much the same thing as a relay- but importantly, vacuum tubes have no moving parts. This meant there was less wear, and more importantly, they could switch thousands of times per second. These **triode vacuum tubes** would become the basis of radio, long distance telephone, and many other electronic devices for nearly a half century.
           
The vacuum tubes were a big improvement over mechanical relays but they weren't perfect because they were kind of fragile and could burn out like light bulbs. Also, vacuum tubes were expensive initially – a radio set often used just one, but a computer might require hundreds or thousands of electrical switches. But by the 1940s, their cost and reliability had improved to the point where they became feasible for use in computers….at least by people with **deep pockets**, like governments. _This marked the shift from electro-mechanical computing to electronic computing._

The first large-scale use of vacuum tubes for computing was **_the Colossus MK I_**, designed by engineer **_Tommy Flowers_** and completed in December of 1943. The Colossus was installed at **_Bletchley Park_** in the UK, and helped to decrypt Nazi communications. This may sound **familiar** because two years prior **_Alan Turing_**, often called the father of computer science, had created an **electromechanical** device, also at Bletchley Park, called the **_Bombe_**. It was an electromechanical machine designed to break Nazi **_Enigma_** **codes**, but the Bombe wasn't technically a computer. Anyway, the first version of **Colossus** contained 1,600 vacuum tubes, ten **Colossi** were built to help with **code-breaking**.           

Colossus is regarded as the first programmable, electronic computer. **Programming** was done by plugging hundreds of wires into plugboards, **sort of** like old school telephone switchboards, in order to set up the computer to perform the right operations. So while "programmable", it still had to be configured to perform a specific computation. The **_Electronic Numerical Integrator and Calculator_** - or **_ENIAC_** -completed a few years later in 1946 at the University of Pennsylvania. Designed by **_John Mauchly_** and **_J. Presper Eckert_**, this was the world's _first truly general purpose, programmable, electronic computer._ ENIAC could perform 5000 ten-digit additions or subtractions per second, many, many times faster than any machine that came before it. It was **operational** for ten years, and is estimated to have done more arithmetic than the entire human race up to that point. But with that many vacuum tubes failures were common, and ENIAC was generally only operational for about half a day at a time before breaking down.           

By the 1950's, even vacuum-tube-based computing was reaching its limits. The US Air Force's **_AN/FSQ-7_** computer, which was completed in 1955, was part of the "**_SAGE_**" air defense computer system. To reduce cost and size, as well as improve reliability and speed, a radical new electronic switch would be needed. In 1947, Bell Laboratory scientists **_John Bardeen_**, **_Walter Brattain_**, and **_William Shockley_** invented the **transistor** and a whole new **era** of computing was born!

The physics behind transistors is pretty complex, relying on **quantum** mechanics. A transistor is just like a relay or vacuum tube- it's a switch that can be opened or closed by applying electrical power via a control wire. Typically, transistors have two electrodes separated by a material that sometimes can **conduct** **electricity**, and other times **resist** it- a **semiconductor**. In this case, the control wire attaches to a "gate" electrode. By changing the electrical charge of the gate, the **conductivity** of the semiconducting material can be manipulated, allowing current to flow or be stopped. Even the very first transistor at Bell Labs showed **tremendous** **promise**- it could switch between on and off **states** 10,000 times per second. Further, unlike vacuum tubes made of glass and with fragile components, transistors were solid material known as a solid state component.        

The transistors could be made smaller than the smallest possible relays or vacuum tubes. This led to **dramatically** smaller and cheaper computers, like the **_IBM 608_**, released in 1957– _the first fully transistor-powered, commercially-available computer._ It contained 3000 transistors and could perform 4,500 additions, or roughly 80 multiplications or divisions every second. IBM soon **transitioned** all of its computing products to transistors, bringing transistor-based computers into offices, and eventually, homes.

Today, computers use transistors that are smaller than 50 **nanometers** in size - for reference, a sheet of paper is roughly 100,000 nanometers thick. And they're not only **incredibly** small, they're super fast - they can switch states millions of times per second and can run for decades. A lot of this transistor and semiconductor development happened in the _Santa Clara Valley_, between _San Francisco_ and _San Jose_, _California_. As the most common material used to create semiconductors is **silicon**, this **region** soon became known as _Silicon Valley_. Even **_William Shockley_** moved there, founding _Shockley Semiconductor_, whose employees later founded _Fairchild Semiconductors_, whose employees later founded **_Intel_** - the world's largest computer chip maker today.