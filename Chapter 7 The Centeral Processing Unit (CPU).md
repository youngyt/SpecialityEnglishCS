---
title:Chapter 7 The Centeral Processing Unit (CPU)
---
# 7.1 the Central Processing Unit
In previous chapters, we've already made a **Arithmetic and Logic Unit**, which takes in binary numbers and performs calculations, and we've made two types of computer **memory**: Registers -- small, linear chunks of memory, useful for storing a single value and then we scaled up and made some RAM, a larger bank of memory that can store a lot of numbers located at different addresses. Now it's time to put it all together and build ourselves the heart of any computer, but without any of the emotional baggage that comes with human hearts. For computers, this is the **Central Processing Unit**, most commonly called the **CPU**. 

A CPU's job is to execute programs. Programs, like Microsoft Office, Safari, or your [[beloved]] copy of Half Life: 2, are made up of *a series of individual operations*, called [[instructions]] because they "instruct" the computer what to do. If these are mathematical instructions, like add or subtract, the CPU will configure its ALU to do the mathematical operation. Or it might be a memory instruction, in which case the CPU will talk with memory to read and write values. 

# 7.2 Microarchitecture
There are a lot of parts in a CPU, so we're going to [[lay it out]] piece by piece, building up as we go. We'll focus on functional blocks, rather than showing every single wire. When we do connect two components with a line, this is an abstraction for all of the necessary wires. This high level view is called the **microarchitecture**. 

Firstly, we're going to need some memory. To keep things simple, we'll assume it only has 16 memory locations, each containing 8 bits. Let's also give our processor four 8-bit memory registers, labeled A, B, C and D which will be used to [[temporarily]] store and [[manipulate]] values. We already know that data can be stored in memory as binary values and programs can be stored in memory too. We can assign an ID to each instruction supported by our CPU. 

In our [[hypothetical]] example, we use the first four bits to store the "operation code", or **opcode** for short. The final four bits [[specify]] where the data for that operation should come from - this could be registers or an address in memory. We also need two more registers to complete our CPU. First, we need a register to keep track of where we are in a program. For this, we use an **instruction address register**, which as the name suggests, stores the memory address of the [[current]] instruction. And then we need the other register to store the current instruction, which we'll call the **instruction register**. When we first [[boot up]] our computer, all of our registers start at 0. 

# 7.3 Three [[phase]]s of instructions execution 
As an example, we've[[ initialize]]d our RAM with a simple computer program. The first phase of a CPU's operation is called the **[[fetch]] [[phase]]**. This is where we [[retrieve]] our first instruction. First, we wire our **Instruction Address Register** to our RAM [[module]]. The register's value is 0, so the RAM returns whatever value is stored in address 0. In this case, 0010 1110. Then this value is copied into our **instruction register**. Now that we've fetched an instruction from memory, we need to figure out what that instruction is so we can execute it. That is run it. Not kill it. This is called the **decode phase**. In this case the opcode, which is the first four bits, is: 0010. This opcode [[corresponds]] to the "LOAD A" instruction, which loads a value from RAM into Register A. The RAM address is the last four bits of our instruction which are 1110, or 14 in decimal. Next, instructions are decoded and interpreted by a Control Unit. Like everything else we've built, it is made out of logic gates too. For example, to [[recognize]] a LOAD A instruction, we need a circuit that checks if the opcode matches 0010 which we can do with a handful of logic gates. Now that we know what instruction we're dealing with, we can go ahead and perform that instruction which is the beginning of the **execute phase**! Using the output of our LOAD_A checking circuit, we can turn on the RAM's read enable line and send in address 14. The RAM retrieves the value at that address, which is 00000011, or 3 in decimal. Now, because this is a LOAD_A instruction, we want that value to only be saved into Register A and not any of the other registers. So if we connect the RAM's data wires to our four data registers, we can use our LOAD_A check circuit to enable the write enable only for Register A. And there you have it -- we've successfully loaded the value at RAM address 14 into Register A. 

We've completed the instruction, so we can turn all of our wires off, and we are ready to fetch the next instruction in memory. To do this, we [[increment]] the Instruction Address Register by 1 which completes the execute phase. LOAD_A is just one of several possible instructions that our CPU can execute. Different instructions are decoded by different logic circuits, which configure the CPU's components to perform that action.  The Control Unit is comparable to the conductor of an [[orchestra]], directing all of the different parts of the CPU. Having completed one full fetch/decode/execute cycle, we're ready to start all over again, beginning with the fetch phase. 

# 7.4 CPU Clock
We just transitioned the CPU through its fetch, decode and execute phases manually. The responsibility of keeping the CPU ticking along falls to a component called the **clock**. As its name suggests, the clock triggers an electrical signal at a precise and regular [[interval]]. Its signal is used by the Control Unit to [[advance]] the internal operation of the CPU, keeping everything in [[lock-step]] - like the dude on a Roman Galley drumming rhythmically at the front, keeping all the rowers [[synchronize]]d... or a metronome. 

Of course you can't go too fast, because even electricity takes some time to travel down wires and for the signal to settle. The speed at which a CPU can carry out each step of the fetch-decode-execute cycle is called its **Clock Speed**. This speed is measured in **Hertz** - a unit of [[frequency]]. One Hertz means one cycle per second. The very first, single-chip CPU was the Intel 4004, a 4-bit CPU released in 1971. Its microarchitecture is actually pretty similar to our example CPU. Despite being the first processor of its kind, it had a [[mind-blowing]] clock speed of 740 KiloHertz -- that's 740 thousand cycles per second. You might think that's fast, but it's nothing compared to the processors that we use today. One MegaHertz is one million clock cycles per second, and the computer or even phone that you are using right now is no doubt a few GigaHertz -- that's BILLIONs of CPU cycles every single second. 

Also, you may have heard of people **overclocking** their computers. This is when you modify the clock to speed up the [[tempo]] of the CPU -- like the drummer speeds up, when the Roman Galley needs to ram another ship. Chip makers often design CPUs with enough tolerance to handle a little bit of overclocking, but too much can either overheat the CPU, or produce [[gobbledygook]] as the signals fall behind the clock. Although you don't hear very much about **underclocking**, sometimes it's not necessary to run the processor at full speed. By slowing the CPU down, you can save a lot of power, which is important for computers that run on batteries, like laptops and smartphones. To meet these needs, many modern processors can increase or decrease their clock speed based on demand, which is called **[[dynamic]] frequency scaling**. So, with the addition of a clock, our CPU is complete. 